{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters-21578 collection\n",
    "\n",
    "Reuters-21578 is arguably the most commonly used collection for text classification during the last two decades.\n",
    "It has been used in some of the most influential papers on the field. \n",
    "\n",
    "This dataset contains structured information about newswire articles that can be assigned to several classes, \n",
    "therefore making this a multi-label problem. It has a highly skewed distribution of documents over categories, \n",
    "where a large proportion of documents belong to few topics.\n",
    "\n",
    "The collection originally consists of 21,578 documents, including documents without topics and typographical errors. \n",
    "For this reason, a subset and split of the collection is traditionally used. This split also focus only on the \n",
    "categories that have at least one document in the training set and the test set. The dataset has 90 categories with a \n",
    "training set of 7769 documents and a test set of 3019 documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Stats\n",
    "\n",
    "Understanding your collection is always the first step of any data science problem. Lets have a quick look at the Reuters collection and its documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 10788\n",
      "Total train documents: 7769\n",
      "Total test documents: 3019\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "# List of document ids\n",
    "documents = reuters.fileids()\n",
    "print(\"Documents: {}\".format(len(documents)))\n",
    "\n",
    "# Train documents\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"),documents))\n",
    "print(\"Total train documents: {}\".format(len(train_docs_id)))\n",
    "\n",
    "# Test documents\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"),documents))\n",
    "print(\"Total test documents: {}\".format(len(test_docs_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRENCH FREE MARKET CEREAL EXPORT BIDS DETAILED\n",
      "  French operators have requested licences\n",
      "  to export 675,500 tonnes of maize, 245,000 tonnes of barley,\n",
      "  22,000 tonnes of soft bread wheat and 20,000 tonnes of feed\n",
      "  wheat at today's European Community tender, traders said.\n",
      "      Rebates requested ranged from 127.75 to 132.50 European\n",
      "  Currency Units a tonne for maize, 136.00 to 141.00 Ecus a tonne\n",
      "  for barley and 134.25 to 141.81 Ecus for bread wheat, while\n",
      "  rebates requested for feed wheat were 137.65 Ecus, they said.\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "['barley', 'corn', 'grain', 'wheat']\n"
     ]
    }
   ],
   "source": [
    "# Example of a document (with multiple labels)\n",
    "doc = 'training/9865'\n",
    "print(reuters.raw(doc))\n",
    "print()\n",
    "print(reuters.categories(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 90\n",
      "\n",
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "# List of categories\n",
    "categories = reuters.categories()\n",
    "print(\"Number of categories: {}\".format(len(categories)))\n",
    "print()\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common categories\n",
      "[('earn', 3964),\n",
      " ('acq', 2369),\n",
      " ('money-fx', 717),\n",
      " ('grain', 582),\n",
      " ('crude', 578)]\n",
      "\n",
      "Least common categories\n",
      "[('castor-oil', 2),\n",
      " ('groundnut-oil', 2),\n",
      " ('lin-oil', 2),\n",
      " ('rye', 2),\n",
      " ('sun-meal', 2)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Documents per category\n",
    "category_distribution = [(category, len(reuters.fileids(category)))\n",
    "                        for category in categories]\n",
    "category_distribution = sorted(category_distribution,key = itemgetter(1),reverse = True)\n",
    "\n",
    "print(\"Most common categories\")\n",
    "pprint(category_distribution[:5])\n",
    "print()\n",
    "print(\"Least common categories\")\n",
    "pprint(category_distribution[-5:])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Learn and transform train documents\n",
    "vectorised_train_documents = vectorizer.fit_transform(train_docs)\n",
    "vectorised_test_documents = vectorizer.transform(test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform multilabel labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(reuters.categories(doc_id) \n",
    "                                for doc_id in train_docs_id)\n",
    "test_labels = mlb.transform(reuters.categories(doc_id)\n",
    "                           for doc_id in test_docs_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of labels assigned: 3126\n"
     ]
    }
   ],
   "source": [
    "classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "classifier.fit(vectorised_train_documents,train_labels)\n",
    "predictions = classifier.predict(vectorised_test_documents)\n",
    "\n",
    "print(\"Numbers of labels assigned: {}\".format(sum([sum(prediction)\n",
    "                                                  for prediction in predictions])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well have we done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro-average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.9516954574536148, Recall: 0.7946047008547008, F1-measure: 0.8660844250363902\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(test_labels, predictions, average = 'micro')\n",
    "recall = recall_score(test_labels, predictions, average = 'micro')\n",
    "f1 = f1_score(test_labels, predictions, average = 'micro')\n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {}, Recall: {}, F1-measure: {}\".format(precision,recall,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average quality numbers\n",
      "Precision: 0.6305, Recall: 0.3715, F1-measure: 0.4451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(test_labels, predictions, average='macro')\n",
    "recall = recall_score(test_labels, predictions, average='macro')\n",
    "f1 = f1_score(test_labels, predictions, average='macro')\n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, \n",
    "                                                                     recall, \n",
    "                                                                     f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this weird warning message about \"Precision being ill-defined?\"\n",
    "\n",
    "Some metrics can be in a position of indeterminate value when for instance, the classifier decides to not classify any articles in a specific category. This would imply a 0/0 precision. Scikit learn warns us about this, and reports back this quality as 0.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
